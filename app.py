import os
import shutil
import streamlit as st
from dotenv import load_dotenv

from rag_pipeline import RAGConfig, read_pdfs_to_docs, chunk_docs, get_vectordb, index_documents, as_retriever
from rag_graph import build_rag_graph

load_dotenv()

st.set_page_config(page_title="RAG R√©vision", page_icon="üìö", layout="wide")
st.title("üìö Assistant IA de r√©vision (RAG)")

# ---- Config env
DEFAULT_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")
CHROMA_DIR = os.getenv("CHROMA_DIR", "data/chroma")

cfg = RAGConfig(chroma_dir=CHROMA_DIR)

# ---- Session state
if "vectordb" not in st.session_state:
    st.session_state.vectordb = get_vectordb(cfg)

if "graph" not in st.session_state:
    st.session_state.graph = build_rag_graph()

if "messages" not in st.session_state:
    st.session_state.messages = []

st.sidebar.header("‚öôÔ∏è Param√®tres")
model_name = st.sidebar.text_input("Mod√®le Groq", value=DEFAULT_MODEL)
k = st.sidebar.slider("k (passages r√©cup√©r√©s)", 2, 8, 4)

st.sidebar.markdown("---")
if st.sidebar.button("üßπ R√©initialiser la base (Chroma)"):
    # Supprime la base persist√©e (utile pendant dev)
    try:
        if os.path.exists(CHROMA_DIR):
            shutil.rmtree(CHROMA_DIR)
        st.session_state.vectordb = get_vectordb(cfg)
        st.success("Base Chroma r√©initialis√©e ‚úÖ")
    except Exception as e:
        st.error(f"Erreur reset: {e}")

st.markdown("### 1) Upload des PDFs")
uploaded = st.file_uploader("Ajoute tes PDFs (ex: ceux que tu as g√©n√©r√©s)", type=["pdf"], accept_multiple_files=True)

colA, colB = st.columns([1, 2])
with colA:
    if st.button("üîé Indexer", disabled=not uploaded):
        with st.spinner("Lecture ‚Üí d√©coupage ‚Üí embeddings ‚Üí indexation‚Ä¶"):
            docs = read_pdfs_to_docs(uploaded)
            if not docs:
                st.error("Aucun texte lisible trouv√© dans les PDFs (si PDF scann√©, il faut OCR).")
            else:
                chunks = chunk_docs(docs, cfg)
                n = index_documents(st.session_state.vectordb, chunks)
                st.success(f"Indexation termin√©e ‚úÖ  ({n} chunks ajout√©s)")

with colB:
    st.info(
        "Conseil d√©mo : uploade les 4 PDFs (IA, ML, Algo, OS), indexe, puis pose des questions. "
        "Le bot r√©pondra avec une section 'Sources'."
    )

st.markdown("### 2) Poser des questions")
question = st.text_input("Ta question", placeholder="Ex: Explique la diff√©rence entre processus et thread.")

if st.button("‚û°Ô∏è R√©pondre", disabled=not question.strip()):
    # retriever
    retriever = as_retriever(st.session_state.vectordb, k=k)

    with st.spinner("Retrieval + g√©n√©ration‚Ä¶"):
        result = st.session_state.graph.invoke(
            {
                "question": question.strip(),
                "retriever": retriever,
                "k": k,
                "model_name": model_name,
                "messages": st.session_state.messages,
            }
        )
        st.session_state.messages = result.get("messages", st.session_state.messages)
        answer = result.get("answer", "")

    st.subheader("‚úÖ R√©ponse")
    st.write(answer)

st.markdown("### üßæ Historique")
for m in st.session_state.messages[-10:]:
    role = "üë§" if m.type == "human" else "ü§ñ" if m.type == "ai" else "‚ÑπÔ∏è"
    st.markdown(f"**{role} {m.type.upper()}** : {m.content}")
