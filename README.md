# ğŸ“š Assistant IA de rÃ©vision (RAG)
### LangChain + LangGraph + Streamlit + ChromaDB

---

## ğŸŒ Application en ligne

**L'application est dÃ©ployÃ©e et accessible gratuitement :**  
ğŸ‘‰ **[https://ia-cours-assistant.streamlit.app](https://ia-cours-assistant.streamlit.app)**

Vous pouvez l'utiliser directement sans installation locale !

---

## ğŸ“– Description

**Assistant IA de rÃ©vision** est une application intelligente de type **RAG (Retrieval Augmented Generation)** qui permet de **discuter avec vos documents PDF de cours**.

### ğŸ¯ FonctionnalitÃ©s principales

âœ… **Indexation intelligente de documents**
- Upload de fichiers PDF (cours, notes, supports pÃ©dagogiques)
- DÃ©coupage automatique en chunks pertinents
- GÃ©nÃ©ration d'embeddings vectoriels
- Stockage dans une base vectorielle ChromaDB persistante

âœ… **Recherche sÃ©mantique avancÃ©e**
- RÃ©cupÃ©ration des passages les plus pertinents par rapport Ã  votre question
- ParamÃ¨tre `k` ajustable pour contrÃ´ler le nombre de passages rÃ©cupÃ©rÃ©s

âœ… **GÃ©nÃ©ration de rÃ©ponses contextuelles**
- Utilisation d'un LLM puissant (Groq API) pour gÃ©nÃ©rer des rÃ©ponses prÃ©cises
- RÃ©ponses basÃ©es **uniquement** sur le contenu de vos documents
- Ajout automatique des sources (document + numÃ©ro de page)

âœ… **Interface intuitive**
- Interface web moderne dÃ©veloppÃ©e avec Streamlit
- Historique de conversation
- ParamÃ¨tres personnalisables (modÃ¨le LLM, nombre de passages)

---

## ğŸš€ Comment utiliser l'application

### Ã‰tape 1 : Uploader vos PDFs

1. Cliquez sur **"Ajoute tes PDFs"** dans la section **"1) Upload des PDFs"**
2. SÃ©lectionnez un ou plusieurs fichiers PDF depuis votre ordinateur
3. Les formats acceptÃ©s : `.pdf`

### Ã‰tape 2 : Indexer les documents

1. Une fois vos PDFs uploadÃ©s, cliquez sur le bouton **ğŸ” Indexer**
2. L'application va :
   - Lire le contenu des PDFs
   - DÃ©couper le texte en chunks (segments cohÃ©rents)
   - GÃ©nÃ©rer des embeddings vectoriels
   - Les stocker dans ChromaDB
3. Un message de confirmation apparaÃ®tra : `Indexation terminÃ©e âœ… (X chunks ajoutÃ©s)`

### Ã‰tape 3 : Poser vos questions

1. Dans la section **"2) Poser des questions"**, tapez votre question
2. Exemples de questions :
   - *"Explique la diffÃ©rence entre processus et thread"*
   - *"Quels sont les algorithmes de tri et leurs complexitÃ©s ?"*
   - *"Qu'est-ce que le Machine Learning supervisÃ© ?"*
3. Cliquez sur **â¡ï¸ RÃ©pondre**
4. L'assistant va :
   - Rechercher les passages pertinents dans vos documents
   - GÃ©nÃ©rer une rÃ©ponse basÃ©e sur ces passages
   - Afficher les sources utilisÃ©es

### Ã‰tape 4 : Consulter l'historique

- L'historique des questions/rÃ©ponses s'affiche en bas de page
- Permet de suivre votre session de rÃ©vision

---

## âš™ï¸ ParamÃ¨tres disponibles

### Dans la barre latÃ©rale :

- **ModÃ¨le Groq** : Choisir le modÃ¨le LLM (par dÃ©faut : `llama-3.1-8b-instant`)
- **k (passages rÃ©cupÃ©rÃ©s)** : Nombre de passages Ã  rÃ©cupÃ©rer (2-8, dÃ©faut: 4)
- **ğŸ§¹ RÃ©initialiser la base** : Supprimer tous les documents indexÃ©s et repartir Ã  zÃ©ro

---

## ğŸ› ï¸ Stack technique

| Composant | Technologie |
|-----------|-------------|
| **Interface utilisateur** | Streamlit |
| **ğŸ”— Framework RAG** | **LangChain** |
| **ğŸ”€ Orchestration** | **LangGraph** (workflow retrieve â†’ generate) |
| **Base vectorielle** | ChromaDB (persistÃ©e localement) |
| **Embeddings** | Sentence-Transformers (`all-MiniLM-L6-v2`) |
| **LLM** | Groq API (`llama-3.1-8b-instant`) |
| **Extraction PDF** | PyPDF |

---

## ğŸ”— Pourquoi LangChain ?

**LangChain** est le framework de rÃ©fÃ©rence pour construire des applications LLM. Dans ce projet, il est utilisÃ© pour :

### ğŸ¯ Gestion des embeddings et du retrieval
- **IntÃ©gration ChromaDB** : LangChain fournit une abstraction Ã©lÃ©gante pour interagir avec ChromaDB via `langchain-chroma`
- **Embeddings uniformes** : Utilisation de `sentence-transformers` via l'API standardisÃ©e de LangChain
- **Retrievers configurables** : SystÃ¨me de retrieval modulaire avec paramÃ¨tre `k` ajustable

### ğŸ“ Text Splitters intelligents
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# DÃ©coupage intelligent qui prÃ©serve le contexte
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)
```

### ğŸ¤– Abstraction des LLMs
- **Multi-providers** : Facile de basculer entre Groq, OpenAI, Anthropic...
- **API unifiÃ©e** : MÃªme interface quel que soit le provider
- **Gestion des prompts** : SystemMessage, HumanMessage, AIMessage standardisÃ©s

### ğŸ“š Gestion des documents
- **Document loaders** : Extraction PDF avec mÃ©tadonnÃ©es (source, page)
- **SchÃ©ma Document** : Structure standardisÃ©e `{page_content, metadata}`
- **ChaÃ®nes composables** : Pipeline retrieval â†’ formatting â†’ generation

---

## ğŸ”€ Pourquoi LangGraph ?

**LangGraph** est le framework de nouvelle gÃ©nÃ©ration de LangChain pour orchestrer des workflows complexes. Il apporte :

### ğŸ­ Architecture Ã  base de graphes

Notre application utilise un **workflow en 2 nÅ“uds** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”
â”‚  START  â”‚  -->  â”‚ RETRIEVE â”‚  -->  â”‚ GEN â”‚  -->  END
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜
```

#### Node 1 : **RETRIEVE**
```python
def retrieve_node(state: RAGState) -> RAGState:
    retriever = state["retriever"]
    question = state["question"]
    docs = retriever.get_relevant_documents(question)
    return {"docs": docs}
```
- RÃ©cupÃ¨re les documents pertinents via le retriever LangChain
- Met Ã  jour l'Ã©tat du graphe avec les documents trouvÃ©s

#### Node 2 : **GENERATE**
```python
def generate_node(state: RAGState) -> RAGState:
    llm = make_llm(state["model_name"])
    context = _format_context(state["docs"])
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=f"CONTEXTE:\n{context}\n\nQUESTION: {state['question']}")
    ]
    
    response = llm.invoke(messages)
    return {"answer": response.content}
```
- Formate le contexte des documents rÃ©cupÃ©rÃ©s
- Construit le prompt avec systÃ¨me + contexte + question
- Invoque le LLM et retourne la rÃ©ponse

### âœ¨ Avantages de LangGraph

#### ğŸ”„ Ã‰tat partagÃ© typÃ©
```python
class RAGState(TypedDict):
    messages: List[BaseMessage]  # Historique
    question: str                # Question courante
    docs: List[Document]         # Docs rÃ©cupÃ©rÃ©s
    answer: str                  # RÃ©ponse finale
    retriever: object            # Retriever
    k: int                       # Nombre de passages
    model_name: str              # ModÃ¨le LLM
```
- Ã‰tat fortement typÃ© Ã©vitant les erreurs
- PartagÃ© entre tous les nÅ“uds du graphe
- Immutable et traceable

#### ğŸ¯ ModularitÃ© et extensibilitÃ©
- **Ajout facile de nÅ“uds** : Ex. un nÅ“ud de re-ranking, de validation, de cache
- **Conditional edges** : Routage dynamique selon l'Ã©tat
- **Parallel execution** : PossibilitÃ© d'exÃ©cuter plusieurs nÅ“uds en parallÃ¨le

#### ğŸ” ObservabilitÃ©
- Chaque transition est tracÃ©e
- Debug facile du workflow
- IntÃ©gration avec LangSmith pour le monitoring

#### ğŸš€ Ã‰volutions possibles

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   RETRIEVE   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   RE-RANK    â”‚
        â”‚           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚
        â”‚           â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚   GENERATE   â”‚
        â”‚           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚
        â”‚           â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   VALIDATE   â”‚
         (retry)    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ†š Comparaison avec LCEL (LangChain Expression Language)

| Aspect | LCEL | LangGraph |
|--------|------|-----------|
| **SimplicitÃ©** | ChaÃ®nes simples | Workflows complexes |
| **Ã‰tat** | Passage de variables | Ã‰tat partagÃ© global |
| **ConditionnalitÃ©** | LimitÃ©e | Routes conditionnelles |
| **Cycles** | Impossible | Support natif |
| **Debugging** | Difficile | Inspection d'Ã©tat |
| **Use case** | RAG basique | RAG avancÃ©, agents |

---

## ğŸ—ï¸ Architecture technique dÃ©taillÃ©e

### Pipeline complet (indexation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF(s)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ PyPDF
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents  â”‚ (LangChain Document schema)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ RecursiveCharacterTextSplitter
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Chunks    â”‚ (1000 chars, overlap 200)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Sentence-Transformers
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embeddings â”‚ (384-dim vectors)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ langchain-chroma
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChromaDB   â”‚ (persisted)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline complet (question-rÃ©ponse)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Question   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LangGraph Workflow    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  RETRIEVE Node  â”‚    â”‚  â† Retriever LangChain
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â† ChromaDB similarity search
â”‚           â”‚             â”‚  â† Top k documents
â”‚           â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  GENERATE Node  â”‚    â”‚  â† Format context
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â† Build prompt (System + Context + Question)
â”‚           â”‚             â”‚  â† ChatGroq LLM
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RÃ©ponse + Src  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


## ğŸ’» Installation locale

Si vous souhaitez exÃ©cuter l'application en local :

### PrÃ©requis
- Python 3.9 ou supÃ©rieur
- ClÃ© API Groq (gratuite sur [console.groq.com](https://console.groq.com))

### Instructions

1. **Cloner le projet**
```bash
git clone <url-du-repo>
cd assitance_cours_IA
```

2. **CrÃ©er un environnement virtuel**
```bash
python -m venv venv

# Windows:
venv\Scripts\activate

# Mac/Linux:
source venv/bin/activate
```

3. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

4. **Configuration**

CrÃ©ez un fichier `.env` Ã  la racine du projet :
```env
GROQ_API_KEY=votre_clÃ©_api_groq
GROQ_MODEL=llama-3.1-8b-instant
CHROMA_DIR=data/chroma
```

5. **Lancer l'application**
```bash
streamlit run app.py
```

L'application sera accessible sur `http://localhost:8501`

---

## ğŸ“ Structure du projet

```
assitance_cours_IA/
â”‚
â”œâ”€â”€ app.py                  # Interface Streamlit principale
â”œâ”€â”€ rag_pipeline.py         # Pipeline RAG (indexation, retrieval)
â”œâ”€â”€ rag_graph.py            # Graph LangGraph (workflow retrieve â†’ generate)
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ Dockerfile              # Configuration Docker
â”œâ”€â”€ README.md               # Ce fichier
â”‚
â””â”€â”€ data/
    â””â”€â”€ chroma/             # Base vectorielle ChromaDB (persistÃ©e)
```

---

## ğŸ§ª Exemple d'utilisation

**Question :** *"Qu'est-ce que le gradient descent en Machine Learning ?"*

**RÃ©ponse gÃ©nÃ©rÃ©e :**
> Le gradient descent (descente de gradient) est un algorithme d'optimisation utilisÃ© pour minimiser une fonction de coÃ»t. Il calcule le gradient (dÃ©rivÃ©e) de la fonction par rapport aux paramÃ¨tres et met Ã  jour ces paramÃ¨tres dans la direction opposÃ©e au gradient...
>
> **Sources :**
> - ML_cours.pdf - page 12
> - ML_cours.pdf - page 13

---

## ğŸ” SÃ©curitÃ© et confidentialitÃ©

- Les documents uploadÃ©s sont traitÃ©s localement ou dans votre session Streamlit Cloud
- Les donnÃ©es ne sont pas partagÃ©es avec des tiers
- La clÃ© API Groq est stockÃ©e de maniÃ¨re sÃ©curisÃ©e (variables d'environnement)

---

## ğŸ‘¨â€ğŸ’» Auteur

Mondher Ben Youssef
Mehdi Jegham
Nabil Ghazouani 
Jouhayna Cheikh
Selim Khelifa